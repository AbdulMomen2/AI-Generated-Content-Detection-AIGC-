{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17406,
     "status": "ok",
     "timestamp": 1750197026956,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "72AIns0_mmeM",
    "outputId": "a0c4f9da-fd12-4030-dda4-e55b6fdf5261"
   },
   "outputs": [],
   "source": [
    "pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRKw5At9luMN"
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import io # Import the io module\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import timm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset # Import load_dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUK-vpJTRBzW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561,
     "referenced_widgets": [
      "62f26bd1c3ff49b899b6e365fed1620b",
      "8cc27e0d1ffa4a78a2d2ed888b123b80",
      "57b6c8d31e424e57879a263bf6d356ae",
      "fbf122b50c6a4eae9c2381d3e6abd7b2",
      "851146766ef14d8685e8728c107a4903",
      "23850b0bfb4343c689af6f5d3b4e8b97",
      "86eedc5a747e4591979db346bd47b5c3",
      "14ab4b93b3e54e5894aa84883c7067fa",
      "87bf23f52889429090ebf0cf905d707f",
      "6d4034e37f5444e18fc771fa0be995df",
      "2067184c975d46aba6a10c04299aa680",
      "10ea6f46ad6f4bca845f1f3df80c4f07",
      "dc728ad8f19b440d9db70f967dab503b",
      "b2ecd51072624ad5a0c612aea7ce9d0a",
      "64c0eb6b5e06471d8d6728d39b281c82",
      "8f4ad9608eed4b3a89b070ef3c733dbf",
      "8edac1a53ca844b7b69c3f426fbda18e",
      "2b9b55db16b147ce90cfb8d5843e312c",
      "255f52910af448e49edbd9a4d454201d",
      "e0c1ab9b7a454cc6bb865db8fbac5ffe",
      "c67398ba1706472d9a9f3fb5b50a397e",
      "df60c089ad844564a26cc0e39ea92ea8",
      "793cb59d8e804ae88732f8caa62aacfc",
      "c6facd54572f48ad8944d6b31e98a3f8",
      "216c73dab1194b1f8f82d43c1cfba0d1",
      "63221ffbadb5428fa2cd64bc62f75980",
      "c2a850753a15404bb17a1cf91bafd502",
      "eaed30359e1346a3b8f3f89add79cfbf",
      "8571cc214b5b4d778079a5cb8f083ea4",
      "0e8fdf13a9464a1faea642226432365e",
      "fce882d7941a4f9e8e93f920feeaecc4",
      "eab5a4caa32f4e128920eb0b4936e719",
      "9a316fa7eaef46f9b011659fbf373177",
      "f8e38858ec74422bab3fa28fae80131d",
      "23c6382d61d4415c9de484a9e87029f2",
      "46c4dda3300c47ada67e3d2cf18abda7",
      "6db617b827ed45b3845c478d51a180f2",
      "0cfc736fc23a4a28a38375fc26253c74",
      "be2457f84bba444e9d18044e5a205969",
      "cd978583fa0645c98d9f50fef011058d",
      "b4254d4d4f8944bc8996e134ea8c3588",
      "25084aeb15a04c148814df57c05ee178",
      "0f00975fde474555b4c77b2c004bd4bd",
      "69532f741ffa4fcfa776de23d50aeca9",
      "8139ec1d80f84edbb9db84b5bacbcda9",
      "6daf984633984a4ca8373657f8f5d984",
      "6acdbda458d748ab844fcb5fe3290538",
      "e71e2dc80f1347c7873a27de348170da",
      "f0071f2850084c35be7a32cbe4b01c2f",
      "5bc1679aea144df9ad9422f41352b4ae",
      "081f788b991b4392a6db56c5258bde83",
      "57516309789d40278ca04d5210fe72c0",
      "0b415283ae3642d8a60046dcf5b3a689",
      "5a450dbb857847f480b45bba96d7db69",
      "7f71ce42e4f34d6f93968b9fe2a08590",
      "b9ee846b7acd4c969c214d6e5d1a77b6",
      "029ba032774c4bea9312c3ed38e1c2de",
      "5a3575d5a8c94a00b5c3ebfce72bc6e7",
      "ef99be15493f4da1ba823d9087099547",
      "eb3f4b47434a4a58bba66f4bfb6bc0c4",
      "68d6b350d37b48928cb7b7c10f51cf9e",
      "51fd8a8272324bf3bca375264ef09941",
      "3fc586d559ff4f3ca09a76fcf5e443ba",
      "67f619fdbc394e4186d16538b44c9613",
      "b5661d16c6114ca1b12fab920c8c96f3",
      "3b7a2a25921f478ba26fc9b9aff6bb6d",
      "e9b4e989bd6a4fa1add5492fb1afe407",
      "a02a7c6deea04ef7ae5c67734eece297",
      "0e26e12ae23c4eb4b2803b5dec5f543d",
      "f72eccdeed614de7ab0d474b0276be4f",
      "353db849432a4def8c8485453a8942ac",
      "fe6efdfb9ae644988554d301eb50815e",
      "8e2cb600abc84dfea89a5b25bfe6988c",
      "67f38e6b55c546ef80497350eea5d945",
      "c261aa5c4108420198db9306f1305ea2",
      "88aeb161994642e8bf5f27f24254f9da",
      "ec1ba53917ea4cd7a92986529d1f9936",
      "a774236a4901490cbe193155e8b0a8f2",
      "79b37e29556f464ba15ee93cef755fde",
      "542d18784b6949b4a334c4f9a2fface0",
      "b28766d9d64f4859b688425376f6f947",
      "86c6b0bb6c8442b7a0e6a0d08b15fb8b",
      "1083545203ec4dccb364ecf80d59cc3b",
      "38ae0e62661540f4bf031410a075c8c6",
      "a5f2107dc0ad4810acf83c859f7a9b2f",
      "005451c13fd140a1a22d48fccaaa287b",
      "92c2769c20be4773a9ad2239ef28fbbd",
      "1e4e6f451c784f5d9bf63e6dc9c5290f",
      "36db4f87b6ec42619c51109ab657fb81",
      "622c757ef9004e38bbac23b833f9b93b",
      "0c7dbeb560a84405ba599520192fa231",
      "d7d593cdf4b64e3f8ee1061fb269c158",
      "284f09ce2ee04b4e9672a66c8b9ddf7a",
      "25fc9ed057254398ae0665d5070919ba",
      "c6c55567e83d471d9e173b3d72499f9a",
      "795a4f604987443e9e04cb2ac5a698e1",
      "96efe7b3b8a64ba38f62c40dc547379d",
      "87908d91a24a4d82acd7d719b03cff87",
      "fa00a7f70e824236885f547862213ff3",
      "a3df4b68307a4ef88b78e5eff5b34101",
      "e1ab3f08f43e47abaa96983f0b713726",
      "bbc37bd464e044ce9bb022dcfeb77c09",
      "3a6d5f1c9fab4a33be9a263a1da32dee",
      "b593cb0826a84599a58cb2612f8c6439",
      "27af0bf4091940819193cbe80a3f3a1f",
      "af062ae03c434047bee8f81f964a4f94",
      "d6ab5910fc194fd78fb809b4629f9165",
      "bea4ac467286483583ac98a77f1846f1",
      "33a48000a7134ab7bb43bf823635fdac",
      "cb5b9d4216b24870bbc59fa30255f728",
      "1359c1b3164140fb93a4409e7762c9e1",
      "787dec26accb4ba8ab9d6d02465acca8",
      "a080aede62734b73bc92a1a5c26cb1c2",
      "34912d1c6023419da0c3e70f7161b769",
      "4998cd98d1114a719dd3aa0dbe0028df",
      "62b2179c2d734e4ca1e53499196f1cff",
      "ccb5f0b77a7046b08d93fe2b9d515c99",
      "da4ebf0c6ee44f0cb3e63f4d9008acd2",
      "f562c79a054d44c1ba0b9a34612b9882",
      "936d957c5a914dbfa927eee572a9289a",
      "1c3bbd7f28d24604973f628be393783a",
      "08fba8dcc18b4763a5a6c2d55e9defed",
      "b6da90a83ffe42abb6099f09cd84f6f7",
      "45558af338bd4929a9f98cbbafe44f2c",
      "a17d2b75349c4c37832f8048242438c2",
      "56c853558671486bab447b2e4b590fa9",
      "2aba19ae81474c6592f3fbe91a714c7d",
      "2b5f1db193614826ad4eb93d614c8e8d",
      "ec41bea8796f4a0a8e0e048a4633aab0",
      "2358c840055b4d46b692d9ec99e0c586",
      "f9f2685fa88843cfb5f43a3856013977",
      "e868940a074d42399da8630ac5215398",
      "0794c32580e445b7beb4945b3acea67a",
      "187018e3a55b40a9b8904e3ee0a6a7b1",
      "7ed86eedaf464060b539dc8e94cf51ad",
      "19c4cbb7afb64f9d93bdea5ca41e1103",
      "ab85470fa5f84d0a9a87b4e7ba745e54",
      "716aa87829894986a255b3fe53eea31c",
      "a897b3ff5f974ed298cd1e6af14f335f",
      "6bde0570043a4a6298b2426660e71e3d",
      "36acc5d14ab44d50b56e590d59904b37",
      "d9da7ef87ed24172a9a709aaedbc2b41",
      "768bbe2f1af94988b3f580cb14d4b0b6",
      "de32a62985014d509b31c2a895c93b11",
      "cc709d04f5db448b9149080bac762285",
      "72f36f9b22664e3dbf1034cf4496a017",
      "987c4f15b81e43248967e26ade891989",
      "8049e30e2dc44311916489be4395b472",
      "de15e1900bf24662b665f17e5f6422bc",
      "fc668ea238a04a928fdab600f857534f",
      "09e3e2b4877246ca9915269e054d7cda",
      "f566ac4ad31f4b74903de501b6cb5104",
      "f5fe7c8b67f54f82a10759e253facc3f",
      "4d3f4e7ba31e4cac9e5a03b19082f8ef",
      "ad55ffb1636c4c24b9e49ba18197bdeb",
      "c51e5dad2a4d4e9abfd1a0d5419a1a75",
      "d3537cacfe254c4b99ecffc7f7795c24",
      "3e1a2c220e414b9383bc3020c6221b56",
      "c6a07b9dfd5e49e5bf8f3fd94d2405a7",
      "99ee99b939b641fc9ce5889f4650d7b2",
      "7eb3e5e708e14a229d0f4061a7be8c75",
      "6ad81d1c873d470987650ea5b3b4666c",
      "22fba7c33c8244088a3f0e982d26089f",
      "dce91fe13ef74b2084076e73d7f20c74",
      "ccbab0f32ade4da7892de1adfc2acba4",
      "b844d2d7e851497b9469bcab69b7e185",
      "9c6ae02f95104ab598e94158945aa768",
      "bd1fe555b9d1427192ba2bc74a027fcd",
      "2cbe9be491524334813cf6762ede388b",
      "3cf725d534ce4e9cbf9c15de6596210d",
      "7d2dd9c927234ca780d25bb3c87e48fb",
      "7407bc900c8647f384516b543d4f77e3",
      "3a4821af78914b38a1f6555993435076",
      "85898e9f6e72470b83d2adb50f7fae95",
      "d2ec3678bfe24a6e843eddcc93725aa8",
      "c3c201f752e74822ab2ad1a81bed1db7",
      "a5bca70d3d04414b825e0e4617ddc7fd",
      "577bc63782dd485a9e82a5eb5a7ee8a6",
      "87f5f312458b433193d932cebc7eb3af",
      "63519cd61e074a229cbf4150680f807a",
      "7f74917310e847bbaf8b89bfbd153b01",
      "17bc478caaba4fe9b6baa37fc80e15cb",
      "080974f58efe4a0aafff7bca4cbc59e1",
      "7f5f32e773614477b75d44fdd4fae919",
      "18e1853606064b1ba93ae12556dea999",
      "3037cc8565014ff7be7f033bcd4e03ae",
      "4e2de165588344de89611f314245b892"
     ]
    },
    "executionInfo": {
     "elapsed": 151606,
     "status": "ok",
     "timestamp": 1750197211566,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "J0p816vljPwF",
    "outputId": "de16c653-085c-459b-d728-7de5e320fead"
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"anson-huang/mirage-news\", download_mode=\"force_redownload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3349,
     "status": "ok",
     "timestamp": 1750199695369,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "9S2hzrJPGBys",
    "outputId": "706cc0a9-2b51-41fd-ef6d-f5f5a80b52b2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_parquet('/root/.cache/huggingface/hub/datasets--anson-huang--mirage-news/snapshots/b5a7e734850b4ec623ddee018a1d9e097fe248ef/data/train-00000-of-00001.parquet')\n",
    "print(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1TGiStMjP1s"
   },
   "outputs": [],
   "source": [
    "class MIRAGE_Ensemble_Dataset(Dataset):\n",
    "    def __init__(self, parquet_path, tokenizer, max_token_len=128, image_transform=None):\n",
    "        self.data = pd.read_parquet(parquet_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        self.image_transform = image_transform\n",
    "        # Filter out rows with missing text, image, or label\n",
    "        self.data = self.data.dropna(subset=['text', 'image', 'label'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        text = str(row['text'])\n",
    "        label = torch.tensor(row['label'], dtype=torch.long)\n",
    "\n",
    "        # Tokenize text\n",
    "        tokens = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_token_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        input_ids = tokens['input_ids'].squeeze(0)\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0)\n",
    "\n",
    "        # Load image\n",
    "        image_data = row['image']\n",
    "        image = None\n",
    "\n",
    "        if isinstance(image_data, dict):\n",
    "            try:\n",
    "                image_bytes = image_data['bytes']\n",
    "                image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "            except Exception as e:\n",
    "                print(f\"[Warning] Failed to load image from bytes at index {idx}: {e}\")\n",
    "        elif isinstance(image_data, str):\n",
    "            try:\n",
    "                image = Image.open(image_data).convert('RGB')\n",
    "            except FileNotFoundError:\n",
    "                print(f\"[Warning] Image file not found at path {image_data} (index {idx}).\")\n",
    "        else:\n",
    "            print(f\"[Warning] Unexpected image data type at index {idx}: {type(image_data)}\")\n",
    "\n",
    "        # Apply transforms or fallback\n",
    "        if image is not None and self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        elif image is None:\n",
    "            # Fallback to zero tensor with the expected size after transform\n",
    "            if self.image_transform:\n",
    "                dummy_image = Image.new('RGB', (224, 224))\n",
    "                image = self.image_transform(dummy_image)\n",
    "            else:\n",
    "                image = torch.zeros((3, 224, 224), dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'image': image,\n",
    "            'labels': label,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bh8YnDeIjPzd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class TextModelWrapper(nn.Module):\n",
    "    def __init__(self, hf_model_name):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(hf_model_name, return_dict=True)\n",
    "        self.hidden = nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size)\n",
    "        self.classifier = nn.Linear(self.model.config.hidden_size, 1)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Use pooler_output if available, else average last hidden states\n",
    "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "            pooled_output = outputs.pooler_output\n",
    "        else:\n",
    "            pooled_output = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        pooled_output = self.hidden(pooled_output)\n",
    "        pooled_output = torch.relu(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_func(logits.squeeze(1), labels.float())  # BCEWithLogitsLoss expects float labels\n",
    "\n",
    "        return loss, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3dz1wfrjPxs"
   },
   "outputs": [],
   "source": [
    "class ImageModelWrapper(nn.Module):\n",
    "    def __init__(self, hf_model_name, device='cpu'):\n",
    "        super().__init__()\n",
    "        # Load ViT backbone without classifier (num_classes=0 means no head)\n",
    "        self.vit = timm.create_model('vit_base_patch16_224', pretrained=False, num_classes=0)\n",
    "        self.classifier = nn.Linear(self.vit.num_features, 1)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Try loading the custom pretrained weights from Hugging Face\n",
    "        try:\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(\n",
    "                f\"https://huggingface.co/{hf_model_name}/resolve/main/pytorch_model.bin\",\n",
    "                map_location=device,\n",
    "            )\n",
    "\n",
    "            # Load base ViT weights (ignore classifier keys)\n",
    "            vit_state_dict = {k: v for k, v in checkpoint.items() if 'classifier' not in k}\n",
    "            missing_keys, unexpected_keys = self.vit.load_state_dict(vit_state_dict, strict=False)\n",
    "\n",
    "            if missing_keys:\n",
    "                print(f\"[Warning] Missing keys when loading ViT: {missing_keys}\")\n",
    "            if unexpected_keys:\n",
    "                print(f\"[Warning] Unexpected keys in ViT: {unexpected_keys}\")\n",
    "\n",
    "            # Try loading classifier weights if present\n",
    "            classifier_state_dict = {k.replace('classifier.', ''): v for k, v in checkpoint.items() if k.startswith('classifier.')}\n",
    "            if classifier_state_dict:\n",
    "                try:\n",
    "                    self.classifier.load_state_dict(classifier_state_dict, strict=True)\n",
    "                    print(\"[Info] Successfully loaded classifier weights.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[Warning] Failed to load classifier weights: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Failed to load checkpoint from {hf_model_name}: {e}\")\n",
    "            print(\"[Info] Using default-initialized weights.\")\n",
    "\n",
    "    def forward(self, images, labels=None):\n",
    "        features = self.vit(images)\n",
    "        features = self.dropout(features)\n",
    "        logits = self.classifier(features)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_func(logits.squeeze(1), labels.float())  # Ensure logits match shape of labels\n",
    "\n",
    "        return loss, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "eaa9187aa37b4d97a625425bfc7b7461",
      "213a97ed7a964f739e4e82d809c5cb86",
      "bf072b0d9f23429cab2fb2f16598afd9",
      "120dd2e3149a4016960ab1fe3b6eb934",
      "beb6e01d20414a4ba83ae4e0f67405e8",
      "de54de42af5d41659239593ccdfcdc4d",
      "1b3c72298b1741fe9fe30fbc803cbebf",
      "58beb4a9495d4e41b0aa949872b5380b",
      "0f68a6a86c5e44c992cf7ee3a08a4c0b",
      "163a534a99ce4e7abcab3500f33a06c0",
      "2c77e2438d81417a8cef2ee2c3e48c90",
      "76afc02c011443cebc14ebaa792c1314",
      "2ad3139d671b47e8be07b311b650f2ba",
      "ced38da53dc84444b7b044fb490bcd07",
      "269ed39d3ddb414a88d10cbab7dc3b96",
      "1ae97370373140b0b027c187b9031118",
      "b54bf1be34e8449bbe92c2023223bcbd",
      "c487c53d5ce84eab967321b12caeace6",
      "2a7791dfc9d14db18bca23bcd0cd91f2",
      "05e8400dcb334c9db632da7b77a71ff0",
      "e6aed9019bc248a2936bf233ddde2bb4",
      "504e3cd2eeca451d84a82b151457a803",
      "56133dea9361439fbeb3d65f5aa51b42",
      "cf1291467f2c46398feecfbf8c067d45",
      "f7e61d5b57bd4b5294027b803b8003c9",
      "90738379281d4911b871a85d6f62e6ab",
      "cd88cbcdf5814559bc7f9e4b227f7338",
      "b07d832f33d84f4f8d3463afe8e787e8",
      "b0f79842af784c48a6d854e9bb2cb686",
      "aef75137617c4fc89be9c5590311b3c4",
      "3a876b6b950644da892102a2fc541770",
      "02cb8099142b4a2f9b143b78989e0e64",
      "73fd197ee21f4b199acade4e6d48e961",
      "2deb5967b451422abb169e5114fa9acd",
      "531c6de469b646b08aa884530400100f",
      "9faece4593cc4b4d922903df4f7484fd",
      "01fca8a30c4a4c5e814b393154f605dd",
      "80af2fdf79fe46d2a3f048204bf992ba",
      "ecc7f38258824766803d1a41229e030b",
      "854951b6f4304a619b16b6a8ed0c98ab",
      "de41ea1bbbdc493d9ac9db50e1c46be6",
      "bf28d31f66a64baeab66ac042d10a4f4",
      "867be5e2b37d4f9796e42a94880ec97b",
      "afb76b6c53de4c2897b7d6d39ad5ea05",
      "5a10a34c2a96435892fcfe006de9cb29",
      "1467d11bd30144af99f8fa2d908fc0b8",
      "e18ddf94e40e4db0a186c802a8ef69d9",
      "33f1918eee874858b0331a96d0f7143a",
      "1ba81c6afe4043eaadd658178f6ead9d",
      "17dcf4e056c440e3b2622fd53d690fef",
      "2dbd7e7286454e9fb78f7ec10b6d2996",
      "84c365176288484db952426cc75d1aa0",
      "d2b37148e5644e25905dbc0ac4ddae39",
      "380ff79a0b44423c9758cac2aca1d185",
      "78fcf63617904e20a4fc732c0170e536",
      "482aff2503c64ac6b118a4549e1e3ee9",
      "c112fd1bf2aa47768efcedb950b96dd7",
      "9f3c0582213c42df8609e0c4f9616a87",
      "68c83c3719c8484d80ce5615b41b6761",
      "a5b28e9495014fdd81948f22637447c8",
      "94abc04a78a84cef92fad80191c18077",
      "890910c92f154a2a8c63e9c3079241aa",
      "267c5ec4d0f54641b2c72a7423a3a313",
      "eb3306902588406b9c6027d5cd81f826",
      "191714db9a0f4905bf7e1ba072efbe97",
      "b506d054344f4f4683535b2e703b258a",
      "fc591a5a37034498904b021773cde1dd",
      "3c441dddc3254169b6a97d8b860152f0",
      "1068b7639a2d404ca6159986c7f9fe00",
      "da6e0bfc8e5f404494b55053ce256815",
      "b321cb6b31bb4e4e891e6943108dac66",
      "9325a4f189c341bfaff02d060c39c15e",
      "95c7e2c2a6cc40d288f39f4889fbf4f9",
      "71526cadb951470caa0bc68523454c8a",
      "ef1d845bc58a4a8582ebae1d529afb08",
      "8b371ad23a684d95826acb4c59ed566e",
      "13455996af0c4c80b9627e503da7733c"
     ]
    },
    "executionInfo": {
     "elapsed": 37709,
     "status": "ok",
     "timestamp": 1750197299065,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "jgu5OpannViJ",
    "outputId": "ccb8ad5a-905e-48ba-8d64-c193a14fba70"
   },
   "outputs": [],
   "source": [
    "# Tokenizer and models\n",
    "text_model_name = \"darkam/fakenews-finetuned-distilroberta-base\"\n",
    "image_model_name = \"darkam/vit-mirage-news\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "text_model = TextModelWrapper(text_model_name).to(device)\n",
    "\n",
    "text_model.eval()\n",
    "\n",
    "\n",
    "# Transforms\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2813,
     "status": "ok",
     "timestamp": 1750201548312,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "A3BlQUhOmPgd",
    "outputId": "a2dcab54-2e61-44df-f73e-5a9d4ceeabca"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# Load pretrained ViT from torchvision\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "image_model = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "# Remove the classification head\n",
    "image_model.heads = nn.Identity()\n",
    "\n",
    "image_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETqL5mROjPtz"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the path to the test parquet file\n",
    "test_parquet_path = '/root/.cache/huggingface/hub/datasets--anson-huang--mirage-news/snapshots/b5a7e734850b4ec623ddee018a1d9e097fe248ef/data/validation-00000-of-00001.parquet'\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = MIRAGE_Ensemble_Dataset(\n",
    "    parquet_path=test_parquet_path,\n",
    "    tokenizer=tokenizer,\n",
    "    max_token_len=128,\n",
    "    image_transform=image_transform,\n",
    ")\n",
    "\n",
    "# Wrap the dataset in a DataLoader\n",
    "test_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,  # No shuffling during evaluation\n",
    "    num_workers=4,  # Adjust based on your CPU capability\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHQQVGniAPoU"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7T7WNn18dYk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def calibrate_probs(probs):\n",
    "    \"\"\"Min-max normalize probabilities to [0,1]\"\"\"\n",
    "    return (probs - probs.min()) / (probs.max() - probs.min() + 1e-8)\n",
    "\n",
    "def soft_voting_ensemble_predict(text_model, image_model, dataloader, device,\n",
    "                                text_weight=0.5, image_weight=0.5, threshold=0.5,\n",
    "                                voting_type='soft'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        voting_type: 'soft' for weighted average probs,\n",
    "                     'hard' for majority voting of class preds.\n",
    "    \"\"\"\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    text_model.eval()\n",
    "    image_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Text model output handling\n",
    "            text_output = text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            if isinstance(text_output, tuple):\n",
    "                text_logits = text_output[1]\n",
    "            elif hasattr(text_output, 'logits'):\n",
    "                text_logits = text_output.logits\n",
    "            else:\n",
    "                text_logits = text_output\n",
    "\n",
    "            text_probs = torch.sigmoid(text_logits).squeeze()\n",
    "            text_probs = calibrate_probs(text_probs)\n",
    "\n",
    "            # Image model output\n",
    "            image_logits = image_model(images)\n",
    "            image_probs = torch.softmax(image_logits, dim=1)[:, 1].squeeze()\n",
    "            image_probs = calibrate_probs(image_probs)\n",
    "\n",
    "            if voting_type == 'soft':\n",
    "                # Weighted average probabilities\n",
    "                combined_probs = (text_weight * text_probs) + (image_weight * image_probs)\n",
    "                preds = (combined_probs > threshold).long()\n",
    "\n",
    "            elif voting_type == 'hard':\n",
    "                # Individual binary predictions\n",
    "                text_pred = (text_probs > threshold).long()\n",
    "                image_pred = (image_probs > threshold).long()\n",
    "\n",
    "                # Majority vote: 2 votes needed for positive (class 1)\n",
    "                summed = text_pred + image_pred\n",
    "                preds = (summed >= 1).long()  # positive if any model votes positive\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"voting_type must be 'soft' or 'hard'\")\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 150154,
     "status": "error",
     "timestamp": 1750201075471,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "OcpwXJBnCauI",
    "outputId": "7c6a9331-baa8-464d-9882-714420714d08"
   },
   "outputs": [],
   "source": [
    "\n",
    "# === Example Usage ===\n",
    "text_weight = 0.7\n",
    "image_weight = 0.3\n",
    "threshold = 0.5\n",
    "voting_type = 'soft'  # or 'hard'\n",
    "\n",
    "all_labels, all_preds = soft_voting_ensemble_predict(\n",
    "    text_model, image_model, test_loader, device,\n",
    "    text_weight=text_weight, image_weight=image_weight,\n",
    "    threshold=threshold, voting_type=voting_type\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "all_labels_np = np.array(all_labels)\n",
    "all_preds_np = np.array(all_preds)\n",
    "\n",
    "acc = accuracy_score(all_labels_np, all_preds_np)\n",
    "prec = precision_score(all_labels_np, all_preds_np, zero_division=0)\n",
    "rec = recall_score(all_labels_np, all_preds_np, zero_division=0)\n",
    "f1 = f1_score(all_labels_np, all_preds_np, zero_division=0)\n",
    "\n",
    "print(f\"Ensemble ({voting_type} voting) | Text weight: {text_weight}, Image weight: {image_weight}, Threshold: {threshold}\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUNCN-Js9QNu"
   },
   "outputs": [],
   "source": [
    "def hard_voting_ensemble_predict(text_model, image_model, dataloader, device, threshold=0.3):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    text_model.eval()\n",
    "    image_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Text model probabilities\n",
    "            text_output = text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            text_logits = text_output[1] if isinstance(text_output, tuple) else text_output\n",
    "            text_probs = torch.sigmoid(text_logits).squeeze()\n",
    "            text_pred = (text_probs > threshold).long()\n",
    "\n",
    "            # Image model probabilities\n",
    "            image_logits = image_model(images)\n",
    "            image_probs = torch.softmax(image_logits, dim=1)[:, 1].squeeze()\n",
    "            image_pred = (image_probs > threshold).long()\n",
    "\n",
    "            # Majority vote (2 models, so vote positive if any model predicts positive)\n",
    "            ensemble_pred = (text_pred + image_pred >= 1).long()\n",
    "\n",
    "            all_preds.extend(ensemble_pred.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147588,
     "status": "ok",
     "timestamp": 1750200193441,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "v5i2HJjy9XbL",
    "outputId": "a9b1486b-1bc2-40c0-84d1-9909a636519c"
   },
   "outputs": [],
   "source": [
    "text_weight = 0.9\n",
    "image_weight = 0.1\n",
    "\n",
    "all_labels, all_preds = soft_voting_ensemble_predict(text_model, image_model, test_loader, device, text_weight, image_weight)\n",
    "\n",
    "# Evaluation\n",
    "all_labels_np = np.array(all_labels)\n",
    "all_preds_np = np.array(all_preds)\n",
    "\n",
    "acc = accuracy_score(all_labels_np, all_preds_np)\n",
    "prec = precision_score(all_labels_np, all_preds_np, zero_division=0)\n",
    "rec = recall_score(all_labels_np, all_preds_np, zero_division=0)\n",
    "f1 = f1_score(all_labels_np, all_preds_np, zero_division=0)\n",
    "\n",
    "print(f\"Soft Voting Ensemble (Text Weight: {text_weight}, Image Weight: {image_weight}) Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f} | Recall: {rec:.4f} | F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRePiPPyHqRo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiModalFusionClassifier(nn.Module):\n",
    "    def __init__(self, text_model, image_model, hidden_dim=512):\n",
    "        super(MultiModalFusionClassifier, self).__init__()\n",
    "        self.text_model = text_model\n",
    "        self.image_model = image_model\n",
    "\n",
    "        # Freeze pretrained weights if desired\n",
    "        # for param in self.text_model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # for param in self.image_model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768 + 768, hidden_dim),  # Adjust if your model uses different hidden sizes\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1)  # Binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        # Text encoding\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Access the pooled output based on the text model's output structure\n",
    "        # Assuming the pooled output is the second element in the tuple for models like BERT\n",
    "        text_feat = text_outputs[1] if isinstance(text_outputs, tuple) else text_outputs.pooler_output  # (B, 768)\n",
    "\n",
    "        # Print shape of text_feat for debugging\n",
    "        print(\"Shape of text_feat:\", text_feat.shape)\n",
    "\n",
    "        # Image encoding\n",
    "        image_outputs = self.image_model(images)\n",
    "        # The image model (ViT with head removed) should output the pooled features (CLS token)\n",
    "        # The output of torchvision's ViT with head removed is typically (B, num_patches + 1, hidden_dim)\n",
    "        # The CLS token is at index 0\n",
    "        image_feat = image_outputs[:, 0, :] # (B, 768)\n",
    "\n",
    "        # Concatenate and classify\n",
    "        combined = torch.cat((text_feat, image_feat), dim=1)  # (B, 1536)\n",
    "        logits = self.classifier(combined)\n",
    "        return logits.squeeze()  # (B,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oS-UDIGHwAe"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['labels'].to(device).float()\n",
    "\n",
    "            # Add print statements to inspect the inputs\n",
    "            print(\"Input IDs shape:\", input_ids.shape)\n",
    "            print(\"Attention Mask shape:\", attention_mask.shape)\n",
    "            print(\"Images shape:\", images.shape)\n",
    "\n",
    "            logits = model(input_ids, attention_mask, images)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > threshold).long()\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds)\n",
    "    rec = recall_score(all_labels, all_labels) # Corrected to use all_labels for recall calculation\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Multimodal Fusion Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f} | Recall: {rec:.4f} | F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNGbqATd4nI1SLzuyR623R2",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
