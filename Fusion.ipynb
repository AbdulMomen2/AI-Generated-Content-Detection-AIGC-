{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15704,
     "status": "ok",
     "timestamp": 1751749634958,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "SeGgvTUaaQQT",
    "outputId": "ec50ecb4-d3df-4c56-c6fa-26c69aeb2670"
   },
   "outputs": [],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1751750494416,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "obSwchAdaTGk"
   },
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class MIRAGE_Ensemble_Dataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, image_processor, max_token_len=128):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        text = str(item['text'])\n",
    "        label = torch.tensor(item['label'], dtype=torch.float)\n",
    "        image = item['image']\n",
    "\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        tokens = self.tokenizer(text, max_length=self.max_token_len,\n",
    "                                padding='max_length', truncation=True,\n",
    "                                return_tensors='pt')\n",
    "\n",
    "        image_input = self.image_processor(images=image, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids': tokens['input_ids'].squeeze(0),\n",
    "            'attention_mask': tokens['attention_mask'].squeeze(0),\n",
    "            'pixel_values': image_input['pixel_values'].squeeze(0),\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1751750497059,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "UCIcOr5jaS94"
   },
   "outputs": [],
   "source": [
    "# model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class CrossModalAttentionFusion(nn.Module):\n",
    "    def __init__(self, text_model_name, image_model_name, hidden_dim=768, fusion_dim=512):\n",
    "        super().__init__()\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        self.image_model = AutoModel.from_pretrained(image_model_name)\n",
    "        self.image_to_text_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8, batch_first=True)\n",
    "        self.text_to_image_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=8, batch_first=True)\n",
    "        self.fusion_proj = nn.Linear(2 * hidden_dim, fusion_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(fusion_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values):\n",
    "        text_feats = self.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        text_cls = text_feats[:, 0, :].unsqueeze(1) # CLS token for text\n",
    "\n",
    "        # Image features from the last hidden state, assuming first token is suitable CLS equivalent\n",
    "        # Note: Some vision models might output pooler_output or have a specific way to get global features.\n",
    "        # For ViT, the first token is typically the [CLS] token, similar to BERT.\n",
    "        image_feats = self.image_model(pixel_values=pixel_values).last_hidden_state\n",
    "        image_cls = image_feats[:, 0, :].unsqueeze(1) # CLS token for image\n",
    "\n",
    "        img2txt_attn_output, _ = self.image_to_text_attn(query=image_cls, key=text_feats, value=text_feats)\n",
    "        txt2img_attn_output, _ = self.text_to_image_attn(query=text_cls, key=image_feats, value=image_feats)\n",
    "\n",
    "        # Concatenate and fuse the attention outputs\n",
    "        fused = torch.cat([img2txt_attn_output.squeeze(1), txt2img_attn_output.squeeze(1)], dim=-1)\n",
    "\n",
    "        x = self.fusion_proj(fused)\n",
    "        output = self.classifier(x)\n",
    "        return output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1751750499797,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "4vpfA28zcgx4"
   },
   "outputs": [],
   "source": [
    "# train_eval.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm.auto import tqdm # Import tqdm for progress bars\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, total_preds, total_labels = 0, [], []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_preds = (outputs.detach().cpu().numpy() > 0.5).astype(int)\n",
    "        batch_labels = labels.cpu().numpy().astype(int)\n",
    "\n",
    "        total_preds.extend(batch_preds)\n",
    "        total_labels.extend(batch_labels)\n",
    "\n",
    "        current_avg_loss = total_loss / (batch_idx + 1)\n",
    "        # Calculate accuracy on accumulated labels/preds to reflect overall batch progress\n",
    "        current_acc = accuracy_score(total_labels, total_preds)\n",
    "\n",
    "        progress_bar.set_postfix(loss=f'{current_avg_loss:.4f}', acc=f'{current_acc:.4f}')\n",
    "\n",
    "    acc = accuracy_score(total_labels, total_preds)\n",
    "    f1 = f1_score(total_labels, total_preds)\n",
    "    return total_loss / len(dataloader), acc, f1\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device, desc=\"Validation\"):\n",
    "    model.eval()\n",
    "    total_loss, total_preds, total_labels = 0, [], []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=desc, leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            batch_preds = (outputs.cpu().numpy() > 0.5).astype(int)\n",
    "            batch_labels = labels.cpu().numpy().astype(int)\n",
    "\n",
    "            total_preds.extend(batch_preds)\n",
    "            total_labels.extend(batch_labels)\n",
    "\n",
    "            current_avg_loss = total_loss / (batch_idx + 1)\n",
    "            current_acc = accuracy_score(total_labels, total_preds)\n",
    "\n",
    "            progress_bar.set_postfix(loss=f'{current_avg_loss:.4f}', acc=f'{current_acc:.4f}')\n",
    "\n",
    "    acc = accuracy_score(total_labels, total_preds)\n",
    "    prec = precision_score(total_labels, total_preds)\n",
    "    rec = recall_score(total_labels, total_preds)\n",
    "    f1 = f1_score(total_labels, total_preds)\n",
    "    return total_loss / len(dataloader), acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644,
     "referenced_widgets": [
      "f9052364a4cb4256998c9b0acc73b7a4",
      "77160117183c4efb89f3d0ca2775c91c",
      "749d4cc5961641d6a81ee30995616ef7",
      "05786f65e9814a47acbc04e58b963384",
      "9892ee40d38e4cb5a3de1023e8caabe7",
      "4aa9f5adbe94458196ea89e265daa235",
      "dd7bb7bba16c434bb97edc8fc1371414",
      "3b82e52a03bb4f2387de2963f44cf79d",
      "da1f1cc6b42d449aa378f2a7d60fe170",
      "9449bc31f76c465bb5676b248d281a57",
      "023528fb9c8a48608f36346f1f441f4e",
      "1cb09f800ea54266b3ab5226cd369a05",
      "6d70cc7175f949eeaccf7e25a9e0f9b5",
      "c31a07110b4842d687999a4b90b0fcab",
      "6ee48301e87a4e66a9e08fc7491f633c",
      "b7f417d271b845b588d09a23205583a8",
      "176efac1adea47b9bd9a8359a627fef8",
      "b7eb0b62c0d04ac88c20534a3fb12aea",
      "17a05cc8ee4b451485fc3db3a040fca6",
      "da38af07d27342e38e68fff97b0af247",
      "86d54309b5df4871ba9ff3b86e7d33a3",
      "9d03f28c00ce44d2aec07f0e125aabe9",
      "faf9f10de12e4009b2c375b227f9c2e1",
      "989dd8058aaa4a628b0853ae0075d2f4",
      "8912db5b9ce54449a9ef8f967e76b39f",
      "25e98457bf9746adb17811887e2e27e9",
      "b9c406b276994bb1aa200f8f32d2556f",
      "2696eab9297f4b9fb3398590c98d58d4",
      "a9934d234e204a118c6c3b622b93ca81",
      "136d9104c0a24db28b10b85cecc35239",
      "144093316b634356808fcf3c10192331",
      "67f1b8da098246a8a7bfe7843ea355fb",
      "57e1c9bd9aed4260b3348e22014a9ac8",
      "983fa8e53c51452491fa03a4743e8268",
      "f10b5e1aa5eb4c4d8d9b3ba3c83e92d0",
      "addbb05a16f24062b092a307bd3843fb",
      "0011dd3d2f3b4d939a1e401cd58eb61a",
      "8af9e58e80694451867f5ac5b6c9a8b7",
      "2037a05d8d9d49838882ed172f5601f1",
      "a00f188d69564286a19038c18949c164",
      "8b2739ab1cc346c8adfb780cbf9ca299",
      "7f3b965834574160875884a5628c6881",
      "15b08ff325824920b1bce1a70fe54786",
      "66463e7d689f43f795ad7262a9581524",
      "dbc7f2e55dc84120ba54109f646d4697",
      "2a9033c7bd9e4aa98ea70b76a98dfe03",
      "7664b4df24b04e6e95e3151a78f61502",
      "ef43ffe7acb04aa197210d5116b0b310",
      "192e7df764464b73b9fb5cda82e254cb",
      "0f90f98916b748909671b49705a6a9ea",
      "ccb8dfe68e19443480214b0ec7d2f7d5",
      "f6cc65704b2140a198022b580761068a",
      "14ca047c9a2f4760a6b7a1d23613abda",
      "c8ee7287690143d9a92d2712b2776326",
      "a459eb2e3a3744eb8dadf47b1090fed0",
      "387ffd415f7e4033a0d032f08c0de7cf",
      "6bcfad1effb748b3a6a1e4ca207a266a",
      "ce7984d2adf84a238e9ebccafa7c5a6a",
      "b943a15ea830415bb93054eb5e2a668d",
      "7c33b4b495cb43838230670a90ddcd93",
      "f79a79d1504c4fb390859c0be68130bb",
      "a51220fa51eb4ffe88e5d4c2a8102a88",
      "2e79d3e366014207b7ded0e299ac4ed8",
      "b9a75dbd0387479e82f7ee6d280513ed",
      "437ae7d1661340b18691c76054de0959",
      "c86cd197da454b5b8382102c550bfd63",
      "5668332b9e2b442caea34541abe0ab96",
      "7516fce0051c4241adf073e45c01252a",
      "6b0dc8e39aaa4f5c93b24d70ff258396",
      "0488c3b2bf9c408c9b3c41967c90d1de",
      "30bcc8ad31c5469db40ba50f6ede8312",
      "0ba7219f8f5d4b74bce0a256c5f439ea",
      "0ee7c6c2a4d5407d817ffa198bdadc43",
      "2e0467e313e947fe8f341559b4c5f1f6",
      "464cec49d44d44ddb641d4c416b63960",
      "98b815e49f82451280e8e5cd6807bc7d",
      "9b27f418c2994c49a5ad303461f27085",
      "b7d26436d1614ad38e3efd75abe947c2",
      "9689a97f131441f1b1df46eb2a2f89f7",
      "88815613a65d432aaff994f08a23ca1a",
      "4358c997b08c4eb39d1656035909730c",
      "e04c371c805949b1b37100225107d592",
      "c09747a668da4272a362e4a63155d191",
      "f20cbf668f704857a558738434d9e16a",
      "82da45c74fd744b583c3f2b7b7431384",
      "ecd70ceb3abf428a8aeb1e5d01276aa8",
      "7a9cb9db869b40039cf2116b6c94517d",
      "cd3b23a8a3df45658b5cc68379027c79",
      "60caac441fab4b92b97d78023bf84c51",
      "32df331bae7e4b49bc0d3d1839cd7cff",
      "50dc06c9ed5c4b3eb36cdb4f0a5bbb77",
      "82584bfccfbd419795c98f03beae9152",
      "b3135f09ced04395aa002b90e7c21b89",
      "d800fda63d2e42f58f9bd61a9fb3e5a5",
      "0f7b0538274046f2bbb0bfb4a92835f6",
      "8c41e00fb1f741d3a90164deefafd29e",
      "e80191bce46640ff95cd4653bd7b50f3",
      "26a9380528764fd1807891fc10bd43eb",
      "6c140be5432f42b6a9a7ed582e804621",
      "c5800468dac84b54b2474d8f7d76b9d0",
      "3760e9a005f249a7bbe3f14851fda159",
      "af2a03ca19954878a631091e086b70c3",
      "3cce71f97d594f09b4ee6ecd76ab95cd",
      "c99e30fa84134153aa1a01128c1d55c1",
      "f0af6f0398884155811e3a6afc48f3b9",
      "77671ad33f0b4ec6a53ca118ca93367d",
      "1f2b604c9b1a48328d6dea72a1277c01",
      "d6cf3c67b2e341a581f5ac30af00cc73",
      "ead19211ceb44043bc855f762380f682",
      "90962618e86645be930fc387e8451968",
      "14fecd7f3fae46b5b35a61bb0cda231c",
      "fb6af9ceb38049ea8b01fdb2665db1ee",
      "15401a15eb5442c0a6ae438ed36577c2",
      "9dd3cc0c9be745b5a072b03a7874ad2e",
      "d3d6c9ca6cd84c8c95f4b8067ab17511",
      "758ffad09da944c4a346dab511424af6",
      "16c49981425a414b8ea0b2de2eeaa982",
      "1a9938445bef41608659f1de971b4173",
      "7beefba626b84ad59ba510c1a2d462c8",
      "487499953ea24636b5eeac7b5ca9ab20",
      "c90a1b5186a8418d8868c9407a67f947"
     ]
    },
    "executionInfo": {
     "elapsed": 3339178,
     "status": "ok",
     "timestamp": 1751754765823,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "ZWqcCYWqdh7J",
    "outputId": "a783daa4-df27-4d38-9bec-d8a5b940607e"
   },
   "outputs": [],
   "source": [
    "# main.py (for Colab)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoImageProcessor\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "IN_COLAB = 'google.colab' in str(get_ipython()) if 'get_ipython' in globals() else False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text_model = 'darkam/fakenews-finetuned-distilroberta-base'\n",
    "    image_model = 'google/vit-base-patch16-224-in21k'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(text_model)\n",
    "    image_processor = AutoImageProcessor.from_pretrained(image_model)\n",
    "\n",
    "    print(\"Loading Mirage-News dataset...\")\n",
    "    # This downloads and caches the dataset. It's usually fine.\n",
    "    mirage_news_dataset = load_dataset(\"anson-huang/mirage-news\")\n",
    "\n",
    "    train_dataset = MIRAGE_Ensemble_Dataset(mirage_news_dataset['train'], tokenizer, image_processor)\n",
    "    val_dataset = MIRAGE_Ensemble_Dataset(mirage_news_dataset['validation'], tokenizer, image_processor)\n",
    "    test_dataset = MIRAGE_Ensemble_Dataset(mirage_news_dataset['test2_bbc_dalle'], tokenizer, image_processor)\n",
    "\n",
    "    print(\"Creating DataLoaders...\")\n",
    "\n",
    "    # --- CRITICAL CHANGE FOR COLAB ---\n",
    "    # Set num_workers to 0 to avoid multiprocessing issues in Colab.\n",
    "    # On a local Linux machine, you might experiment with num_workers > 0\n",
    "    # but for Colab, 0 is often the most stable and sometimes even faster.\n",
    "    num_workers_to_use = 0\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=num_workers_to_use)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, num_workers=num_workers_to_use)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, num_workers=num_workers_to_use)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = CrossModalAttentionFusion(text_model, image_model).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    best_f1 = 0\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(\"Starting training loop...\")\n",
    "    for epoch in range(5):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{5} ---\")\n",
    "        train_loss, train_acc, train_f1 = train_model(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc, val_prec, val_rec, val_f1 = evaluate_model(model, val_loader, criterion, device, desc=\"Validation\")\n",
    "\n",
    "        print(f\"  Train Metrics -> Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
    "        print(f\"  Val Metrics   -> Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Prec: {val_prec:.4f}, Rec: {val_rec:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_crossmodal_fusion.pt\")\n",
    "            print(\"  Best model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  F1 score did not improve. Patience: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(\"\\nLoading best model and evaluating on test set...\")\n",
    "    if os.path.exists(\"best_crossmodal_fusion.pt\"):\n",
    "        model.load_state_dict(torch.load(\"best_crossmodal_fusion.pt\"))\n",
    "    else:\n",
    "        print(\"Warning: 'best_crossmodal_fusion.pt' not found. Using current model state for test evaluation.\")\n",
    "\n",
    "    test_loss, test_acc, test_prec, test_rec, test_f1 = evaluate_model(model, test_loader, criterion, device, desc=\"Test\")\n",
    "    print(f\"\\nFinal Test Performance -> Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, Prec: {test_prec:.4f}, Rec: {test_rec:.4f}, F1: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 754,
     "status": "ok",
     "timestamp": 1751754911556,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "ZrGsU5uYupvI",
    "outputId": "f4eb57c7-1ba7-4a99-dc5b-18c058202929"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best_crossmodal_fusion.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2239,
     "status": "ok",
     "timestamp": 1751755059686,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "WfsKfjaTurjZ",
    "outputId": "1da615f0-203f-4b98-88a5-580ceb2c8054"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def show_sample_predictions(model, dataloader, device, class_names=[\"Fake\", \"Real\"], n=12):\n",
    "    model.eval()\n",
    "    shown = 0\n",
    "\n",
    "    # Calculate grid size based on n\n",
    "    cols = min(n, 3) # Limit columns to avoid overly wide plots\n",
    "    rows = math.ceil(n / cols)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].cpu().numpy()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "            preds = (outputs > 0.5).long().cpu().numpy()\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                if shown >= n:\n",
    "                    break\n",
    "                image = batch['pixel_values'][i].cpu().permute(1, 2, 0).numpy()\n",
    "                # Ensure image data is in the correct format for matplotlib\n",
    "                # ViT outputs normalized images, need to reverse this for display\n",
    "                # This is a common issue, depending on how the image_processor normalizes\n",
    "                # A more robust solution might involve storing original images or using a more complex de-normalization\n",
    "                # For this example, we'll assume a simple normalization that can be somewhat reversed/handled\n",
    "                image = (image - image.min()) / (image.max() - image.min())  # Simple normalization for display robustness\n",
    "\n",
    "                true_label = class_names[int(labels[i])]\n",
    "                pred_label = class_names[int(preds[i])]\n",
    "                color = \"green\" if true_label == pred_label else \"red\"\n",
    "\n",
    "                axes[shown].imshow(image)\n",
    "                axes[shown].set_title(f\"Pred: {pred_label} | True: {true_label}\", color=color, fontsize=10) # Reduced font size\n",
    "                axes[shown].axis(\"off\")\n",
    "                shown += 1\n",
    "\n",
    "            if shown >= n:\n",
    "                break\n",
    "\n",
    "    # Hide any unused subplots if the last row is not full\n",
    "    for i in range(shown, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "\n",
    "    plt.suptitle(\"Model Predictions on Test Images\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call this after test evaluation\n",
    "show_sample_predictions(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1751755389963,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "VM-EBp8GvawY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_individual_predictions(model, dataloader, device, n_real=7, n_fake=5, output_dir=\"saved_predictions\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "    saved_real, saved_fake = 0, 0\n",
    "    idx = 0  # to track global index\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].cpu().numpy()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "            preds = (outputs > 0.5).long().cpu().numpy()\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                true = int(labels[i])\n",
    "                pred = int(preds[i])\n",
    "                correct = (true == pred)\n",
    "\n",
    "                # Only save if we still need more of that class\n",
    "                if (true == 1 and saved_real < n_real) or (true == 0 and saved_fake < n_fake):\n",
    "                    image_tensor = batch['pixel_values'][i].cpu()\n",
    "                    image_np = image_tensor.permute(1, 2, 0).numpy()\n",
    "                    image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())\n",
    "                    image_np = (image_np * 255).astype(\"uint8\")\n",
    "                    img = Image.fromarray(image_np)\n",
    "\n",
    "                    label_str = \"real\" if true == 1 else \"fake\"\n",
    "                    pred_str = \"real\" if pred == 1 else \"fake\"\n",
    "                    correctness = \"correct\" if correct else \"wrong\"\n",
    "                    filename = f\"img_{idx}_{label_str}_pred-{pred_str}_{correctness}.png\"\n",
    "                    filepath = os.path.join(output_dir, filename)\n",
    "                    img.save(filepath)\n",
    "\n",
    "                    if true == 1:\n",
    "                        saved_real += 1\n",
    "                    else:\n",
    "                        saved_fake += 1\n",
    "\n",
    "                    idx += 1\n",
    "\n",
    "                if saved_real >= n_real and saved_fake >= n_fake:\n",
    "                    print(f\"✅ Saved {saved_real} real and {saved_fake} fake images to {output_dir}\")\n",
    "                    return\n",
    "\n",
    "    print(f\"⚠️ Only saved {saved_real} real and {saved_fake} fake images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13306,
     "status": "ok",
     "timestamp": 1751755429352,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "ibN_AilIvxM4",
    "outputId": "91ff5f6a-f888-486f-98e6-0cf67e058ce8"
   },
   "outputs": [],
   "source": [
    "save_individual_predictions(model, test_loader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1751755463548,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "ajKs6x-rwxQp",
    "outputId": "7c05e931-b1ea-401e-a414-3f3076e099a4"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Zip the saved_predictions folder\n",
    "shutil.make_archive(\"predictions_zip\", 'zip', \"saved_predictions\")\n",
    "print(\"✅ Zipped as predictions_zip.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1751755477406,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "qHrOHQE4w0lA",
    "outputId": "bee0a2ed-4ca1-4143-b847-648349fd60de"
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Display a download link\n",
    "FileLink(\"predictions_zip.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1751755548635,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "ovNqOp9NxF8p",
    "outputId": "f465157a-9adc-4c6b-878f-232222cec08f"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"predictions_zip.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 73497,
     "status": "ok",
     "timestamp": 1751755798518,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "6MFULpPixw5b",
    "outputId": "92df4c06-3156-4a46-f427-aa1bee66f102"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "def extract_fused_features(model, dataloader, device):\n",
    "    model.eval()\n",
    "    features, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            label = batch['label'].cpu().numpy()\n",
    "\n",
    "            # Forward pass to get fused features (before classification)\n",
    "            text_feats = model.text_model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "            text_cls = text_feats[:, 0, :].unsqueeze(1)\n",
    "            image_feats = model.image_model(pixel_values=pixel_values).last_hidden_state\n",
    "            image_cls = image_feats[:, 0, :].unsqueeze(1)\n",
    "            img2txt_attn_output, _ = model.image_to_text_attn(query=image_cls, key=text_feats, value=text_feats)\n",
    "            txt2img_attn_output, _ = model.text_to_image_attn(query=text_cls, key=image_feats, value=image_feats)\n",
    "            fused = torch.cat([img2txt_attn_output.squeeze(1), txt2img_attn_output.squeeze(1)], dim=-1)\n",
    "            fused = model.fusion_proj(fused)  # shape: (B, fusion_dim)\n",
    "\n",
    "            features.append(fused.cpu().numpy())\n",
    "            labels.extend(label)\n",
    "\n",
    "    return np.concatenate(features, axis=0), np.array(labels)\n",
    "\n",
    "def plot_tsne(features, labels, title=\"t-SNE of Fused Features\"):\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    features_2d = tsne.fit_transform(features)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(features_2d[labels==0, 0], features_2d[labels==0, 1], c='red', label='Fake', s=10)\n",
    "    plt.scatter(features_2d[labels==1, 0], features_2d[labels==1, 1], c='green', label='Real', s=10)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run after training\n",
    "features, labels = extract_fused_features(model, val_loader, device)\n",
    "plot_tsne(features, labels, title=\"Feature Visualization on Mirage-News (Validation Set)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 879,
     "status": "ok",
     "timestamp": 1751755846315,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "xnZai_RoyOIr"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(model, dataloader, device, class_names=[\"Fake\", \"Real\"]):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].cpu().numpy()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "            preds = (outputs > 0.5).long().cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix - Test Set\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "executionInfo": {
     "elapsed": 61111,
     "status": "ok",
     "timestamp": 1751755999373,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "14220683120135160261"
     },
     "user_tz": -360
    },
    "id": "kn6NVSgPyRNh",
    "outputId": "fed3b3ed-769a-4b85-8c90-7ce139acde1d"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, val_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNsSlCqrZ90RCxUCsaYR9h9",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
