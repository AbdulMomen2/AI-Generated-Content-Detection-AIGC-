{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11381,
     "status": "ok",
     "timestamp": 1748204995279,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "LTVuM3-zD4WU",
    "outputId": "ed46ad98-5e51-4f61-cfb8-8745f12fd5fd"
   },
   "outputs": [],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6185,
     "status": "ok",
     "timestamp": 1748205001467,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "4piKF-YZFF-Q",
    "outputId": "edbe910d-b176-44ad-b2c2-2dbd6e4dd862"
   },
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15008,
     "status": "ok",
     "timestamp": 1748205016476,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "48zIwfy-Dv8l"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1748205016510,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "sF1rwHNLERb0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plot linear data or training and test and predictions (optional)\n",
    "def plot_predictions(\n",
    "    train_data, train_labels, test_data, test_labels, predictions=None\n",
    "):\n",
    "    \"\"\"\n",
    "  Plots linear training data and test data and compares predictions.\n",
    "  \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    # Plot training data in blue\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "\n",
    "    # Plot test data in green\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "\n",
    "    if predictions is not None:\n",
    "        # Plot the predictions in red (predictions were made on the test data)\n",
    "        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "\n",
    "    # Show the legend\n",
    "    plt.legend(prop={\"size\": 14})\n",
    "\n",
    "\n",
    "# Calculate accuracy (a classification metric)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "\n",
    "    Returns:\n",
    "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "\n",
    "def print_train_time(start, end, device=None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format).\n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"\\nTrain time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time\n",
    "\n",
    "\n",
    "# Plot loss curves of a model\n",
    "def plot_loss_curves(results):\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "             \"train_acc\": [...],\n",
    "             \"test_loss\": [...],\n",
    "             \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "    loss = results[\"train_loss\"]\n",
    "    test_loss = results[\"test_loss\"]\n",
    "\n",
    "    accuracy = results[\"train_acc\"]\n",
    "    test_accuracy = results[\"test_acc\"]\n",
    "\n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label=\"train_loss\")\n",
    "    plt.plot(epochs, test_loss, label=\"test_loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label=\"train_accuracy\")\n",
    "    plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "from typing import List\n",
    "import torchvision\n",
    "\n",
    "\n",
    "def pred_and_plot_image(\n",
    "    model: torch.nn.Module,\n",
    "    image_path: str,\n",
    "    class_names: List[str] = None,\n",
    "    transform=None,\n",
    "    device: torch.device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "\n",
    "\n",
    "    # 1. Load in image and convert the tensor values to float32\n",
    "    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n",
    "\n",
    "    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n",
    "    target_image = target_image / 255.0\n",
    "\n",
    "    # 3. Transform if necessary\n",
    "    if transform:\n",
    "        target_image = transform(target_image)\n",
    "\n",
    "    # 4. Make sure the model is on the target device\n",
    "    model.to(device)\n",
    "\n",
    "    # 5. Turn on model evaluation mode and inference mode\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Add an extra dimension to the image\n",
    "        target_image = target_image.unsqueeze(dim=0)\n",
    "\n",
    "        # Make a prediction on image with an extra dimension and send it to the target device\n",
    "        target_image_pred = model(target_image.to(device))\n",
    "\n",
    "    # 6. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "\n",
    "    # 7. Convert prediction probabilities -> prediction labels\n",
    "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
    "\n",
    "    # 8. Plot the image alongside the prediction and prediction probability\n",
    "    plt.imshow(\n",
    "        target_image.squeeze().permute(1, 2, 0)\n",
    "    )\n",
    "    if class_names:\n",
    "        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
    "    else:\n",
    "        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n",
    "    plt.title(title)\n",
    "    plt.axis(False)\n",
    "\n",
    "def set_seeds(seed: int=42):\n",
    "\n",
    "    # Set the seed for general torch operations\n",
    "    torch.manual_seed(seed)\n",
    "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1748205016545,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "KCniVdzZEq7o",
    "outputId": "22e2111a-287c-4420-bb91-fe33fc7d34f4"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1396,
     "status": "ok",
     "timestamp": 1748205066137,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "_psq1fAcE1Iu"
   },
   "outputs": [],
   "source": [
    "# 1. Get pretrained weights for ViT-Base\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "\n",
    "# 2. Setup a ViT model instance with pretrained weights\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "# 3. Freeze the base parameters\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "class_names = ['real', 'fake']\n",
    "set_seeds()\n",
    "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n",
    "# pretrained_vit # uncomment for model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1101,
     "status": "ok",
     "timestamp": 1748205069707,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "RrJcdFOlE8Gu",
    "outputId": "1ac92587-41d7-4833-e1ab-c3c1fadec47f"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=pretrained_vit,\n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561,
     "referenced_widgets": [
      "1e49b26ee27b4858b1ade63ac1586f50",
      "301d92f9193e4487b4ef4f83430ead50",
      "cfc7398e549d423ea1d9a71077004e3b",
      "aa07684d526e4057b94de344ce415c9d",
      "86c9279ee79648b2a2d0d1c37cd4f2e5",
      "e8821b0263d64cf5b8228d6d3201967c",
      "d654ab7789324e648d06ce5254c9ae69",
      "57522ba9436047bba5d58a9f4509dfa5",
      "c065334b96b44c78b6ac65296d4b2518",
      "f1f571c3561944718dac9582f54d34ee",
      "b1592f461a3a42428d0c6cac63dbe782",
      "5abab20264f6404189ea60a4dace7c05",
      "2a3cf05c6cde434db2b43fe570b73e90",
      "59179fe7137346188630129bfc8abf1c",
      "2f52d4a911854ab6a59168804083f4bd",
      "2261c82e94aa43a88d2500726e56f0cc",
      "6aced79eb7634529806915b0a9588ba9",
      "dfd98eea619d49b797a95536e4067871",
      "39c3d0be05544626972d804f977954f7",
      "f83334fa629a4f0eafbc016a78a71acd",
      "5f57ab30de81483daad57fb563c595b4",
      "dcc63cd1af7244df8f17975b10b2cc5c",
      "8a62ae2d94e14d4eb42ac128d99eca36",
      "4db8c83816cc49aaa7b3950cb9c2f172",
      "8792fcddfbaf4a4393bb8227eb51a81f",
      "9fc1666793ee4da8b032abd6c4dc405e",
      "cae8478f18954dec8df4e9717c017d21",
      "d580e1c40f2f4e1f938d26a3d1312752",
      "3a63702fdaa043709a14a93c232bbdbe",
      "28e76aadd6c84c119b48c48b208d71b4",
      "56750bc51cfb491882e1bb19e1de103f",
      "6125cd53dd1b4cd3936f6ff29309df87",
      "c02e460039fc474991980ae1c167210b",
      "fef09a4cf4fb4f6388f257b7ddd7f5c0",
      "d5d2fd5ffd0542a3b137a8578d3d2cf0",
      "6f8fefba80fe4f61a09dbae7351a75e2",
      "a724ff8f38944b73a1b8a21ae2fc137b",
      "4bf2e58dea8241d88692f444339dad4c",
      "ef26b5a6831842ae85c9d2ff3c3ed937",
      "83e1df9dc9b641128e5a706a408b1486",
      "1a9f702a607e474488f78863d461407b",
      "640ab30a216241f1a611b5c849654295",
      "8f324913a5394ab58bc47dce354e5aba",
      "41a6f40c45e14664b6453e538535483d",
      "654355cb32fa4efbb9eb93a679be2e30",
      "5c76450bc1ad432b854af51177049341",
      "8d110383097b437d8ab39fb0a1d7b7d0",
      "3a8cce4a73694226bf591dfdda563751",
      "5c060d6b94154c7882a0478105c0b91a",
      "52f18aa42af247b9aa6dd0b692f957eb",
      "a886f554681d4d33942ab176d15e2ede",
      "dfa63b8490254a35b7a14cb351eff4ee",
      "13d8221b4b7c4e7691dc690ae7f82a37",
      "90db42894837449a8e7a3a639041daa0",
      "9225af39484c49fdae735760777fca1d",
      "cd987952ad1a4ce2a05b1d3837d3a96f",
      "65e2b05f82cc4f3d8f79bb79febb9880",
      "c2f084b1dce443bc831c5ea53af093fb",
      "b4b7b5b71364442daeec4f05be03b0e4",
      "85da667b0b3a4b918895779c4bcb3769",
      "a83d5d47a8c74a499461571ced6bdb26",
      "f5e0387e07924c0f8f942c8b779aaa62",
      "8bcfc3e557b7476e9b0b7679d3d167c8",
      "819877bbcad040ec924a569149172e27",
      "731d5344013e4a03bcfdd95bff5bb5df",
      "4188226ed2ea499b967e0d297b231030",
      "bd2f6b6eddff4c54b7c06305d9f7fd50",
      "4110a485c43a4dcf93c4cf2b39e6eab3",
      "7a6e3234347a471bbebbcd0d207fa1b5",
      "894ad58ccf34492ba073a10ffab83d49",
      "d24b61f7b78844ddaf9c912add6c4cfa",
      "542a5f2b5fc241a09210108f10049044",
      "472d0786cade4874886b01b12485a99f",
      "cf92f2c6cf734c7b8ce085fa8bc0ffec",
      "074ebf13d3b44e609bf417d9c67e16fe",
      "ff1ccc6951564d099d7511cffc05cab7",
      "6eb674bb61e54aad882a1cd7f00813e7",
      "42ec42a33caf46e8bbd9a5e8ffa89428",
      "f7443208a1cc479694495dd3e9b6c3f0",
      "96c3d034614b4bfc96258866e50ba26a",
      "d1ddfbfb400744c3a1d6d00b46361c28",
      "b6fbe4660c1447c8945d89604596133f",
      "db12f17042824f9d84fc685606863bcb",
      "a68a553ab2f340bb80e495687196a705",
      "8ebd96a5d8354f78b7b7c695e1bac9d1",
      "50eecc7db9224feaac2da90c6d5d6e73",
      "164dccb0cdeb4486b5191a79a30c7fef",
      "6eeaf09bb890404abf7fd714514fbdff",
      "bdc3f5b1bb02491981cce04b1a368cce",
      "61380186805e465c9db3b8d761032b8e",
      "420a363d01fb412f899150cc89850965",
      "638b10638c3a4d25af6741a28452448b",
      "59b4540efcd149cea51f8b1ecf58fc2b",
      "cb6a6016c3b349608f8d8479f8ca1a12",
      "87065386aa5044a58af90f6908766c39",
      "adfb49a95fcf45118738dd3770d8d422",
      "d9a32793b91f4bdf81dc48516e7824be",
      "d255407e88694eb69b1291d658fa4fa2",
      "13241ac8693745ab815cdca9c117aea8",
      "c674bd12ec68452eb92a59d9c3c94b8d",
      "a9f9fd329d974c408d4deb0ea102dae3",
      "9bd3fda20ad34ec69b79caecd4d089fe",
      "3d25d80e0dde42a8a84bbf33c64bead3",
      "28cc04cbb87045c587db533550c7b4ce",
      "1b5238ef77f04f4b8a9a072b9e0962ff",
      "09dd277940464e61a815117203c0bc53",
      "2e5754a2a3f7412fa0ba5e7afd3ffdc7",
      "695e94ee4e0b48009345bf0e0529151f",
      "0fc5bff329f748f6b063acfa60e2ce56",
      "3a57aefc83db4c9b90361d4c0a6a5c54",
      "bcb5e1d608124007b8259a61afe76e43",
      "0244dccdafe04172ab624c0a53a9c8e0",
      "f5d091f32e1c4791b00dbaee0c1a6360",
      "e6c51238a0104884ad5309bfbb0e1cd0",
      "479691e453ce4a428a851c0cb32fefa7",
      "f9f1218f4ade435592c3506262003f03",
      "f5157666fdc04ca6873f8408bef04e62",
      "8ea2b32e687444fb8d29f96190142d8b",
      "407ce1560664432696d313f2e17b9446",
      "fd3adf6696b34dde962b5a05c08befce",
      "3135f9a6f2c4410cb95629a5bb937f7f",
      "0304db6c162e4ba2b740181c46776e6e",
      "a2e2623480394c5eb9f74f1f355e74a3",
      "376f9258cabc4bd69e2e60351b6526a4",
      "4239150346a84660a9c26ee46cff2d6c",
      "3db6267a60b24d4fbe7a73a6d39a1dbc",
      "80bb3e34c3a748aab75332ded4e110e3",
      "b2da164324ad4e608554cdd3b30b2d28",
      "77669eaf774e43c5bd352fd49fdc4a36",
      "dc08b90af7a6452bb69170be3c8f97ef",
      "b810c767ce1d4e5da335fc1bf67e1f8b",
      "853e6b79f80c40329cee913ceabda214",
      "1752e42cf2cc429cb51894e9cdf5196b",
      "05170ecbf55e47c29920638b40a15758",
      "807d3f106ba5441f816b62cd89ab091f",
      "d0813a3f51784975aaa6763c5cafec2a",
      "63cb0556bf1d440ca8ba1bfc1b56a3de",
      "0fde866872114cb4ae67a228df895cc0",
      "26b8424c2bb94c75a20e048da2453600",
      "6754442926ec4957994c1325ac7163c5",
      "ab349a3f41dd44f1bbb5d99059d0885e",
      "89fc5fcfc76e45d584b27e31ec35d2da",
      "2fdd3e2a84104b6eb7798629f557c534",
      "e14296357d1d4994a645c1b05ea7d6bd",
      "19aaa4c9f05841d5adb6a502a148eb6c",
      "742a741500d94b70a7df19b5d9b7ecbb",
      "a5c0337c398649409a4ef0a2a806176f",
      "39229b3df6054a789b2552497be355c6",
      "183e5411f4a6446b8cdb4a54ff09f8d6",
      "76e8c0166f134bbb816d196040cbd6e7",
      "c5dde75480f64d74a7865a48741205d8",
      "a9a394840b11484089bb3d8728e92a60",
      "a451edfa71e3491993aba9ad5b3c5dcd",
      "6dd523c6065445b0b908e11509dda25f",
      "22827f70d7ec4ba7a0b8c0d8b2582f21",
      "ce2778c4c94d47e28e2b57ad0bf32ca2",
      "32b05f5319244c519fb03a0415f4dd52",
      "27648572b2104128bdbf01e0e39af6bf",
      "744c28ab0a734c608b16422eca7e378b",
      "de6b875160374a54a5e288a16433c182",
      "c1ed550b14cf4e6a9dd89cd5c9ae1577",
      "ea6691745a9a4ba29e1379cda2ad9d6d",
      "37cc63fbce574972a437d030fcab4411",
      "2195a3167b2f4c86bb245a8ec62e7b5e",
      "2964c77891af461ebfa8b08cdcbda668",
      "9ce83479af9548edadc077fc3b5e39f2",
      "9b6bc9d934994e43876a357f9e00c39a",
      "e50538fd7bed4b8d9ccbde442bcb7d37",
      "3e1af85dda9d4c95a0f7c2b2bbdfdcbe",
      "a129622c023640b3bb75f3ae39d55c76",
      "251c14019bd84e7693047ea4455dd1f1",
      "26103f8c1fa94b6dba9700e2b7d6e01b",
      "91bad97a68bb4ca48daa463a4c41c585",
      "ca3a8866d0bc475f953c666efa44887e",
      "432110d49e2e43e49193abf8d21e6853",
      "5669b6983aa9461fb3aa9924f4593c4b",
      "6ed66e25efb1497880c0330c58fcf809",
      "c5506786be7f490bb81ad7fc085bec0c",
      "eb991cb324c04e2ba6758188e1569302",
      "ee47d953d7ac499692d8b877adf5c77e",
      "794ffbb626ce48a181a1b0045497e77e",
      "ec58bdef61374009821bab518bf85db6",
      "4b1f5c769399493e87cd294a51c6b47d",
      "2e5fd148d0084cf6b44de342f9763cad",
      "9e79abc0bfdb42b6aa723779e49fca88",
      "ee5b05f28a144171bf0c30065974c265",
      "e071041591514000a4957c00434034fa"
     ]
    },
    "executionInfo": {
     "elapsed": 41652,
     "status": "ok",
     "timestamp": 1748205114179,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "WbV_VpZEFP13",
    "outputId": "f691e6ae-c81a-4576-d6ba-fccc935e79e7"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"anson-huang/mirage-news\", download_mode=\"force_redownload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1748205114193,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "B-QmpqxWFdsM"
   },
   "outputs": [],
   "source": [
    "train_ds = ds[\"train\"]\n",
    "val_ds = ds[\"validation\"]\n",
    "test_ds =ds[\"test1_nyt_mj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1748205114229,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "Fr2U-3V7Fgvv",
    "outputId": "ba2f1bd6-b21e-4a5a-a137-bae6ebca4959"
   },
   "outputs": [],
   "source": [
    "# Get automatic transforms from pretrained ViT weights\n",
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "print(pretrained_vit_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1748205114234,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "eRDdf7c8Fn1R"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# Custom dataset wrapper for HuggingFace Datasets\n",
    "class HFDatasetWrapper(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "\n",
    "        # Convert image to RGB if it's not already\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "def create_dataloaders(\n",
    "    transform: transforms.Compose,\n",
    "    batch_size: int,\n",
    "    num_workers: int = NUM_WORKERS\n",
    "):\n",
    "    # Load the mirage-news dataset\n",
    "    ds = load_dataset(\"anson-huang/mirage-news\", download_mode=\"force_redownload\")\n",
    "\n",
    "    # Wrap the HuggingFace datasets into PyTorch Dataset objects\n",
    "    train_dataset = HFDatasetWrapper(ds[\"train\"], transform=transform)\n",
    "    val_dataset   = HFDatasetWrapper(ds[\"validation\"], transform=transform)\n",
    "    test_dataset  = HFDatasetWrapper(ds[\"test1_nyt_mj\"], transform=transform)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_dataloader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_dataloader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    class_names = ['real', 'fake']  # Since labels are 0 (fake) or 1 (real)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529,
     "referenced_widgets": [
      "2017247a10da4800919525a0d1da0d6b",
      "9086946f631040d1ae7c91f62998e819",
      "74f539880cc943e0b2a967f731179e96",
      "7afcec3182024b32a457abc6ef7b29cb",
      "2f161e6fdc674717b56f6eb951982505",
      "4451c71148db45358c6bd10ddd3129b4",
      "1a427006f19d4d2b880c3ad9d88eb341",
      "67d785480eff48228460fc1be5304c4e",
      "b1db51e403d243919ae3ea7b033a4cb3",
      "f32dd2957f444357b53d1b51f1728516",
      "faefd7bdc7e24e5cb45973cb21563d0f",
      "03bb9786a443465495cf62e2a0143da9",
      "1328634f227f4db09b3bdf2e7d039631",
      "42f3f80695d94a2fa32be92670933205",
      "b24874e8c29a47059326dc78be2f251c",
      "60dc2b09677d45f39bb3635ed53045ee",
      "b81d07bab19d4565b063265dcd46efd3",
      "2bd22afbb14f4daab7ea0c11226edf1c",
      "82ab18b076dc43ed87bf845095200cd3",
      "0d2cb2f9915349e79efc5238ba48ab30",
      "6fcaf5637bef44f18ce0acb01e39cb81",
      "6ba2516a97094b238f2be4a5a3da4833",
      "a80f888ff529467a8520f4d36a20f321",
      "61c93cb8c6684a85bc6161425e7173af",
      "e0231c5b9d824da481b2b81649c35e30",
      "37358dc8324b4cafae8659468553d0e4",
      "ac03a15dba06432090dec77520d8a074",
      "422331d3c2ff47e4a1764fd1aaec5c7e",
      "915ca8950358425ca34e2f055fda40a5",
      "7177d5c92225410c95a9cb224fc4b85e",
      "00515453c455420ca061ba12675b34f8",
      "cd23f778f78c4f7b91f0740004e9d4d2",
      "f2e9a7fba5f0471f8e5617f3e5af3435",
      "f02564c021a34daaa9f536804cc74659",
      "bfa2bcf9e2174cffb1bf6818e665a200",
      "995dd2574510499dacbcc11d0a489d5c",
      "508f18cdd0484029a6bbf78e8b240db6",
      "83756acc3eca423b82a8a9f179a716da",
      "e8f1c5893f6c4cf8bb257bbb6395f629",
      "d3719416c65742b5b9bac69fb145e35f",
      "5b1369f57272426da6508c885dd3428e",
      "ee68120e2a4b4f8a92ff3267aa715b90",
      "57d168be35e8430dbefbd4bef1d150f3",
      "7cc14eae36124566bf62a087d73c8fbc",
      "16420707f56d4647b66a988c6f297ab9",
      "e22a0360bacc4c40954339bc33236701",
      "423bda0dfcb4450a9d9125896ed184c0",
      "58429dade5f249cd88e223e0865ddf91",
      "7641ddd096394b2eba2b590594d32d9a",
      "734a4009612a4cb08293ca34b9ad0d0a",
      "f1f82f5e60134dbbae254b5bfa71b84f",
      "4e579a9d25334551b050342eae7f36a7",
      "077e23a8867946f7ac643003a9c6846c",
      "bd5cfed58a47473ca83b4e8f1c57eb33",
      "4b35c3a71e194392808023e88780566b",
      "ecd9af81c77a456ca033e1b4976c3dc8",
      "4b9dc2a307d643ec95128e9eedf17c7c",
      "a96d4af34037439da0ed6f5a50292bc4",
      "a380c87c49e044b7b46e413f093e607d",
      "2433eaa33b8147c3b1588a7d4051334c",
      "6f471769ffd1456499cb026ca1f70e17",
      "649c2d91878545e98f256ae3fc292818",
      "310f31784bce424b81b5318f8fd67da2",
      "54963fb82c7c43b9a8a0781757ecca2a",
      "a48326c203dd455bac714fc0c1b63ab3",
      "4b06233fa76c4b06b6949514f5b54b45",
      "58ed8cfebd4d40f5ac9b18037ef0a800",
      "0a13fcd05bdd452fbc0bc4f1101afb6c",
      "4c616ef6a4eb474cad29d874d7d43f5f",
      "fff1b52f472a445db38f46d28985c60f",
      "91a72d0937bd46c38de8734b2228853c",
      "7602e63f8d5241488a3ba0569f2aa297",
      "d8ee0041bc224582a77edee6bd7a69d1",
      "5effae1a67674daeb5208406981b2735",
      "ba9466981b6d41fd9dbcfab596759465",
      "e8968ad23d9b4173b1e04631c080c94d",
      "6f1dc51885944d9381c756639fd3215f",
      "93102268a6324a80a6f3b281da971243",
      "5bdd8b2fdab34ee995144f29bc989a1e",
      "3d15d0548085461fb396f3f05d2877a0",
      "aa9135ab2c794cf694f84dd5a1ccb0a8",
      "87039e4d0f8547b689be51cf6faa48d4",
      "8cfa1b7bb19345f7b33c0e40d80d9a12",
      "7d1384bc88d042babe51b36f24dd80db",
      "c71be5bbdc5649969b10cee435a273af",
      "f05afa645b99414b91b6392f9ffcd99c",
      "dc984e2add1643648e2bf1117f71da0f",
      "65926fdec75f4b0dbef1de32a6aaf622",
      "7b48ef2d95a642669bcfff797ffc4bf9",
      "d26ac5162636490da5b8f05e7dae9915",
      "f2fb43774e51403ba8d476194b77b4aa",
      "4bdbfb90cefd4a65a3d21b6c96a67f59",
      "03626a8ff9844f35b325535b785725ae",
      "63ac1d6a223949ceb25fcc407bec88ff",
      "80c3d3ae3a9242ebb895546cfaee35ea",
      "ab7d25d2c69a48c39a2540fcb25efca9",
      "46e6caccdb374db786b931be7613531e",
      "d06ec0a385b0406ba3134bc55fcc379f",
      "afe1f2a6bea8479ebc8fec3ee91509f0",
      "ad7084d538e5499abccdca04f2fb2d53",
      "d53f296781704d7ca41a3829087b0233",
      "ef07218051ee4258abd6f2ac4d8c7a53",
      "04daf09f248a43d0b7b1e9beb25af9cd",
      "8013f9a4831a4cfa88af3b99eb882c34",
      "4b464a4eb4a94b69890c212c45161743",
      "b72224359e0b4793a5ae807d273df13b",
      "e952e5d6587b426697f1de17a5ba4c6a",
      "88a647d477b348cda8fede6f8ab9a6bb",
      "4988daaacbd1472096515714f4135140",
      "6ed379b847384636a9e50c63c13f9e09",
      "e0f37951ca0c4a31bfa33dd99815338f",
      "67a242a4fd1b425eac7ab1fec0322df6",
      "a6c5769cd2ac4cc2b5ee29ce148df45d",
      "961e91b774f9459489b9e2a864d7d8c8",
      "01730aabb0ab46bfbf5911f0e9105d8d",
      "555fca3cbe9b47e5a5aef22b2e57189a",
      "3fa65566b9a74fc3b034ad8300547720",
      "be32a1b0229f439ca0369108f57b31d0",
      "e935a15188594f98a508d3f2ec213fed",
      "069d361a0234422eb38a085920598d26",
      "dc6e86b99ba240f0b016df8b3f62987f",
      "68d0bbca3273414a94cfbc6a2c12a478",
      "f0196d8a1ac041d98d0e7543139c9e5f",
      "7d1810bc610f47a48c86548bef4640a3",
      "fdc87cbf17124025b9f779bb26e20672",
      "bb75bffe0d194d5aaab67a1058564572",
      "fb9daf0e92df4ff1ae3bcf4055f8923b",
      "c5a95d15a7404acea119d33950e6ce32",
      "b4c48f6494a4485190e3c15a9518c868",
      "a235aeb557444dce8676c319160e8022",
      "a2f4b13b42e3423a96eef692885dbc15",
      "f020b4e51d574ec68a836dd8e62dd994",
      "fa24bb67bbb6452aaf63651bab6d8b3b",
      "9ded8469fd664f37aa44733ce5397097",
      "33b11ff6e20c454d97d999d827af5352",
      "58201a2bf8bc431b8be40c90f82886af",
      "3a97d76746064fbd87dcafd53609d7eb",
      "e96c48259f4843c8b96dbccd67bd442a",
      "4c0b53b775df461e963f53a6292a5417",
      "c2fd5a880ff54f5c9fb25bb16fa4ad53",
      "62a11c3f241c4500a1dd683a0b6ee901",
      "b4644d43bb954999ab81514bbcedf8c6",
      "ad486d7a149049ef908aec7cbaf966e6",
      "b7d5bfda7af243a98c26473448dd89c7",
      "fec9d05d44444d33bf738553aac9e996",
      "09c1af5b478c405a874c4e2264d0e839",
      "af4af38c5dc6462d9b2e4415dc1e1f7d",
      "c96e0a07b16f45ac869751bc1e66c0b6",
      "d3e6ba0ab8774abd9b55ac8bd8896bdc",
      "d1713343e2794956adfa34070f345d46",
      "ca025b4b33e445aaad267dbf6e9f7e5e",
      "16a4f05111974f7f9ef580c73b2fe687",
      "96d7ae3f549443188f42b3601f57e954",
      "9aba76d65f574b8d8c6e29b358c27a9f",
      "0c904920218647e0a486a08a8f251cd9",
      "b16e2ea726e044afa1c2da70c5942c47",
      "bd9eea23af214a7ca2e1fc24a9831bf4",
      "8debe4947e7944f6bc5b2adb73ab0865",
      "7547db2924874b5aab2a24b6c86ba6b8",
      "50c92f9e816b4719b4d6e834d9c0edc2",
      "c6510bcaa0b2498bb1ed8e9d28896e32",
      "ef51004e89bb4d6292f15b7bf4d5dcb2",
      "4591391636d248239a0217926b5eb278",
      "89920d521a3c4b4aacc8a31ff6aba2b3",
      "fa80b40cd61449149a029e10aa524bbd",
      "897ae379b03c46cb9f67ed05d9beb9c9",
      "a8f77f66823c480a9078b4426e95c916",
      "ae9553a9f1d64f25a182d747181052eb",
      "1a2bc2fc947741a5b99b4e7db852becc",
      "ba8adc1508114e128ae84c9bc45925f0",
      "8219e47c871b469a898439218841c330",
      "37404f8a096e452288c6514a27b4a183",
      "4a323d6aa3e34825a5521c4c50d82e83",
      "249eec6b68fb4a0f916532d0f1636b71",
      "9cd5620a4b684640bba4135511ea5945",
      "429538fcab07442bb55aff00b1110e44"
     ]
    },
    "executionInfo": {
     "elapsed": 42506,
     "status": "ok",
     "timestamp": 1748205162464,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "EVen0MCMGdA7",
    "outputId": "70f71fac-1f71-4d6a-f337-e9b66a9ab86d"
   },
   "outputs": [],
   "source": [
    "train_dataloader_pretrained, val_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(\n",
    "    transform=pretrained_vit_transforms,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1748205163765,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "vb_HuTcRHhvW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n",
    "\n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Accuracy\n",
    "            test_pred_labels = torch.argmax(torch.softmax(test_pred_logits, dim=1), dim=1)\n",
    "            test_acc += (test_pred_labels == y).sum().item() / len(test_pred_labels)\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc /= len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> Dict[str, List]:\n",
    "\n",
    "    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_loss, test_acc = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223,
     "referenced_widgets": [
      "11ebbfafee994398be70ee8d9a076407",
      "db3c99fee7c94693a68940b86da3be14",
      "da5595262e2e4e82b1bcbc23e5513add",
      "c8f6070d56ab497ba0ae8ebd6e72a8dc",
      "0798415ca798432ca99c15cb25ccd798",
      "98db46c6ec534e9da17b5726cfb15564",
      "a80fee1d9bf94498b1cb74662ba0b70a",
      "165e14afd5ca49ed8280f3c20c6e9041",
      "cee16139e77c4e258dd61d2666955871",
      "e4dc01bb8ab0499695cff902f3a2d16e",
      "d75119e31b164bd59012fd17f30e014d"
     ]
    },
    "executionInfo": {
     "elapsed": 1393982,
     "status": "ok",
     "timestamp": 1748206561173,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "Yv2Pe6NGG061",
    "outputId": "acfb5fbd-a6ad-472e-ab38-4b9ec6231180"
   },
   "outputs": [],
   "source": [
    "# Create optimizer and loss function\n",
    "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
    "                             lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the classifier head of the pretrained ViT feature extractor model\n",
    "set_seeds()\n",
    "pretrained_vit_results = train(model=pretrained_vit,\n",
    "                                      train_dataloader=train_dataloader_pretrained,\n",
    "                                      test_dataloader=test_dataloader_pretrained,\n",
    "                                      optimizer=optimizer,\n",
    "                                      loss_fn=loss_fn,\n",
    "                                      epochs=10,\n",
    "                                      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1128,
     "status": "ok",
     "timestamp": 1748207122246,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "czmUB-o6Q6B9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "    print(\"Confusion Matrix:\\n\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26032,
     "status": "ok",
     "timestamp": 1748207232704,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "0JmAmFZsRB4E",
    "outputId": "ba66cde2-90d0-4538-9bed-3b5f11c3b910"
   },
   "outputs": [],
   "source": [
    "evaluate_model(pretrained_vit, val_dataloader_pretrained, class_names, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1748207331746,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "RUa35chSRe7m",
    "outputId": "d23f7721-19ce-4e04-aa89-6fe7b98f4ab8"
   },
   "outputs": [],
   "source": [
    "plot_loss_curves(pretrained_vit_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3680,
     "status": "ok",
     "timestamp": 1748207422288,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "ETKKC4TXR8-x"
   },
   "outputs": [],
   "source": [
    "torch.save(pretrained_vit.state_dict(), \"vit_image_model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584,
     "referenced_widgets": [
      "7caee17a4f5e4f0192f0ad9c8ee51e2a",
      "d728d3977041420183a9e3d25b0566b9",
      "5eb585b015ac4c19b418b1133faa913f",
      "56fe33b5def9488790ed036b2eb12b95",
      "5b8270c8c3364dcdab42c10993c4f1ce",
      "cbbd04182057439f833ba10b7f4a2ae0",
      "9cab90d631534327babbe4e2752c1899",
      "3812d22be78a4f36ac362af0b9d342f8",
      "6bd840c848444876985bbd0096d62eae",
      "8b37508b415c4cc3a51dce062a96d83b",
      "cba77e2690b44b70914901c02ed3a8fe",
      "276419856942485183e7f3edaecb0311",
      "3438e327c1394900bd0f4c3fbad32143",
      "e9a23b46f921484699a7456f3dcab902",
      "88da05d1825d4f139daffd1a75aeb74c",
      "439c276fe461402e9cea18953f781654",
      "03cf18f53d084187bb31914fef00b96c",
      "84b98bdafab94d6ca4c5d2ce57233a89",
      "98a9887b05004bc39c41c778ed3b6350",
      "ebd600035aa74ea28a3c716914762d0d"
     ]
    },
    "executionInfo": {
     "elapsed": 4905,
     "status": "ok",
     "timestamp": 1748207576774,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "IHPfn-sKSoku",
    "outputId": "66089a55-1144-46c5-ef93-7ea5daeb90ba"
   },
   "outputs": [],
   "source": [
    "!pip install -U huggingface_hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1080,
     "status": "ok",
     "timestamp": 1748207633009,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "rR9tjCODS0KE"
   },
   "outputs": [],
   "source": [
    "model_save_path = \"vit_model.pth\"\n",
    "torch.save(pretrained_vit.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1236,
     "status": "ok",
     "timestamp": 1748209105731,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "vnEbsSMdXS8d"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "import torch\n",
    "import os # Import the os module\n",
    "\n",
    "# Save model\n",
    "model_dir = \"vit-mirage-news\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "pretrained_vit.cpu()\n",
    "torch.save(pretrained_vit.state_dict(), f\"{model_dir}/pytorch_model.bin\")\n",
    "\n",
    "# Save model config (optional but useful)\n",
    "config = {\n",
    "     \"architecture\": \"vit_b_16\",\n",
    "   \"labels\": class_names,\n",
    "   \"input_size\": [3, 224, 224],\n",
    "   \"transform\": str(pretrained_vit_transforms)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f\"{model_dir}/config.json\", \"w\") as f:\n",
    "  json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1748209138292,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "EgLJFoFAYhs_",
    "outputId": "5bc83aa2-c623-42fd-d4b6-6be67734f952"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "repo_id = \"darkam/vit-mirage-news\"\n",
    "create_repo(repo_id, private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "1ea54bd6a358488592b4b5eb406ff4d2",
      "8cfe3dcfb5574b609ccd73e748e4e947",
      "156c276fa3214e79a315061828726636",
      "86135fd71294490c94f4216c7201e8ab",
      "4849f6dddd17467d8d1c4630b869c06f",
      "ffb0c46a30e44b79be7903c75bfb97d5",
      "a503614990724b679d276ca7765c5b36",
      "fddcb3c7857b4c0690376db4b0c5ba4c",
      "49a31f776bac44e4b4c5724a654f43b4",
      "3c2e9acfee3347ba88e277c55b82f859",
      "09fc3201113341f198e65df34b5b6d0f"
     ]
    },
    "executionInfo": {
     "elapsed": 11577,
     "status": "ok",
     "timestamp": 1748209203335,
     "user": {
      "displayName": "Abdul Momen",
      "userId": "17927057704355443558"
     },
     "user_tz": -360
    },
    "id": "459WfLtIYoLA",
    "outputId": "931d7e4c-232e-4d7b-b4fe-44c6febb025e"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_folder\n",
    "\n",
    "upload_folder(\n",
    "  repo_id=repo_id,\n",
    "  folder_path=model_dir,\n",
    "  path_in_repo=\"\",\n",
    "  commit_message=\"Upload ViT model fine-tuned on mirage-news\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPPibbIAHkhSFklpIeOgdC/",
   "gpuType": "T4",
   "mount_file_id": "1rgazMAh48NJPQ9D1NM6L3N3S2YFzQa0w",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
